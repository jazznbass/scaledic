---
title: "Introduction to scaledic"
output: 
  rmarkdown::html_vignette:
    df_print: kable
vignette: >
  %\VignetteIndexEntry{Introduction to scaledic}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
library(knitr)
library(dplyr)
library(tibble)
library(scaledic)
library(stringr)

knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  collapse = TRUE,
  comment = "#>"
)
```

# Note

The following code examples in this package make use of the pipe operator `%>%` (see <https://r4ds.had.co.nz/pipes.html> for more details).  

# How to use scaledic

You need two things:

1. a dataset,
2. a dictionary file.

## A dictionary file

A dictionary file is a table with a row for each variable and a column for each parameter of those variables. Most conviniently, you prepare a dictionary file in a spreadsheet program for later use with datasets.  

The following table shows the parameters and the naming of the columns of a dictionary file: 

```{r tab_dic_param, echo = FALSE}
out <- tribble(
  ~Parameter, ~Meaning, ~Example,
  "item_name", "A short item name", "itrf_1",
  "scale", "Abreviation of the scale the item belongs to", "irtf",
  "subscale", "Abrevation of the sub scale", "int",
  "subscale_2", "Abrevation of the second order sub scale", "shy",
  "scale_label", "Name of the scale", "Integrated Teacher Report Form",
  "subscale_label", "Name of the sub scale", "internalizing problems",
  "subscale_2_label", "Name of the second order sub scale", "shyness",
  "item_label", "Full text of the item", "Vermeidet die Teilnahme an Diskussionen im Unterricht",
  "index", "An index number", "1",
  "values", "Valid response values in an R manner", "1:5 (for integers 1 to 5) 1,2,3 (for integers 1, 2, 3)",
  "value_labels", "Labels for each response value", "0 = nicht; 1 = leicht; 2 = mäßig; 3 = stark",
  "missing", "Missing values", "-888, -999",
  "type", "Data type (factor, integer, float, real)", "integer",
  "weight", "Reversion of item and its weight", "1 (positive), -1 (reverse), 1.5 (positive, weights 1.5 times)",
  "source", "Reference", "Casale et al. (2017)",
  "note", "Further notes", "Item has low discrimination"
)
kable(out, caption = "Columns of a dictionary file")

```

Here is extract of a dic file from the included example:

```{r dic_example}
dic_itrf %>% slice(2:4) %>% kable()
```

## Apply a dictionary file

When you combine a dataset with a dictionary file, each variable in the dataset which corresponds to a variable described in the dictionary will be complemented with the given dictionary information.  
The resulting dataset now is ready to be used with all further `scaledic` functions.  

The `apply_dic` function takes the name of the dataset and the dictionary file and combines them. Thereby replacing missing values with NAs:

```{r apply_dic}
# Here we use the example dataset "dat_itrf" and the example dic file "dic_itrf"
dat <- apply_dic(dat_itrf, dic_itrf)
```

Let us get an overview of all scales in the dataset:

```{r list_scales}
list_scales(dat, labels = TRUE) %>% kable()
```

## Clean raw data

Firstly, we check for invalid values in the dataset (e.g., typos) and replace these with NA:

```{r check_values}
dat <- check_values(dat, replace = NA)
```

Now we impute missing values:

```{r impute_missing, eval = FALSE}
# Imputation for items of the subscale Ext
dat <- impute_missing(dat, subscale == "Ext")

# Imputation for items of the subscale Int
dat <- impute_missing(dat, subscale == "Int")

```

```{r impute_missing_eval, include=FALSE}
dat <- impute_missing(dat, subscale == "Ext")
dat <- impute_missing(dat, subscale == "Int")
```

## Select scales for analyszing

Let us see descriptive statistics for the Internalizing sub scale:
 
```{r descriptives}
dat %>% 
  select_items(subscale == "Int") %>%
  descriptives(round = 1)
```

## See items instead of labels

It is more convenient to see the original items instead of the short labels:

```{r desc_labels}
dat %>% 
  select_items(subscale == "Int") %>%
  rename_items() %>%
  descriptives(round = 1)  %>% 
  kable()
```

And further we analyse the factor structure. Here we use the `rename_item()` function to get a more convenient description.  

```{r exploratory_fa}
dat %>%
  select_items(scale == "ITRF") %>%
  rename_items(pattern = "({reverse}){subscale}_{subscale_2}: {label}", max_chars = 70) %>%
  exploratory_fa(nfactors = 4, cut = 0.4)  %>% kable()
```

and provide item analyses

```{r item_analysis}
scales <- ex_itrf %>% get_scales(
  'APD' = subscale_2 == "APD",
  'OPP' = subscale_2 == "OPP",
  "SW" = subscale_2 == "SW",
  "AD" = subscale_2 == "AD"
)
alpha_table(dat, scales = scales) %>% kable()

```

and even a confirmatory factor analysis with the use of the lavaan package.

```{r lavaan_model}
model <- lavaan_model(scales, orthogonal = FALSE)
```

```{r cat_lavaan_model, comment = "", echo = FALSE}
cat(model)
```

```{r lavaan_cfa, comment = ""}
fit <- lavaan::cfa(model = model, data = dat)
lavaan::summary(fit, fit.measures = TRUE)

```

## Build scale scores

Now we build scores for internalizing and externalizing scales

```{r scores}
dat$itrf_ext <- score_scale(dat, scale == "ITRF" & subscale == "Ext", label = "Externalizing")
dat$itrf_int <- score_scale(dat, scale == "ITRF" & subscale == "Int", label = "Internalizing")
```

and get descriptives for these scores

```{r desc_scores}
dat %>%
  select_scores() %>%
  rename_items() %>%
  descriptives(round = 1)
```

## Lookup norm values from a normtable

Many scales come with norm tables to transform raw-scores into T-scores, percentile ranks etc.

The `lookup_norms` function helps with this conversion.

Firstly, you need a data frame (or Excel table etc) which includes raw-scores and corresponding norm-scores.

Here is an example of such a table:

```{r}
ex_normtable_int %>% kable()
```

Then we need raw-scores from a scale. If they do not exist, you may use the `score_scales` function to add sum scores. Therefore, set the sum argument to `TRUE`. By setting `max_na = 0`, we do not allow missing values in any scale item:

```{r}
dat$raw_int <- score_scale(dat, subscale == "Int", sum = TRUE, max_na = 0)
dat$raw_ext <- score_scale(dat, subscale == "Ext", sum = TRUE, max_na = 0)
```

By default, `lookup_norms` looks for T values:

```{r}
dat$T_int <- lookup_norms(dat$raw_int, normtable = ex_normtable_int)
dat$T_ext <- lookup_norms(dat$raw_ext, normtable = ex_normtable_ext)
```

But this can easily be changed to percentile ranks, if included:

```{r}
dat$PR_int <- lookup_norms(dat$raw_int, normtable = ex_normtable_int, to = "PR")
dat$PR_ext <- lookup_norms(dat$raw_ext, normtable = ex_normtable_ext, to = "PR")
```

```{r}
dat[1:10, c("T_int", "T_ext", "PR_int", "PR_ext")] %>% kable()
```

